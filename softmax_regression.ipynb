{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "ogRWrFng9TOf"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "lZu1lP_n7u2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a573df-5f02-45ff-9368-a31d2da27a51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1-LOTR.txt', '1-game-of-thrones.txt', '1-harry-potter-and-the-chamber-of-secrets.txt', '1-harry-potter-and-the-sorceres-stone.txt', '1-the-lion-the-witch-and-the-wardrobe.txt', '2-a-study-in-scarlet.txt', '2-and-then-there-were-none.txt', '2-murder-on-the-orient-express.txt', '2-the-girl-with-the-dragon-tattoo.txt', '2-the-hound-of-baskervilles.txt', '3-451-farenheit.txt', '3-brave-new-world.txt', '3-divergent.txt', '3-dune.txt', '3-enders-game.txt', '3-the-martian.txt', '4-carrie-stephen-king.txt', '4-dracula.txt', '4-frankenstein.txt', '4-it-by-stephen-king.txt', '4-the-shining.txt', '5-persuasion.txt', '5-pride-and-prejudice.txt', '5-romeo-and-juliet.txt', '5-twilight-new-moon.txt', '5-twilight.txt', '6-moby-dick.txt', '6-the-count-of-monte-cristo.txt', '6-the-three-musketeers.txt', '6-tom-sawyer.txt', '6-treasure-island.txt', '7-a-tale-of-two-cities.txt', '7-memoirs-of-a-geisha.txt', '7-the-book-thief.txt', '7-the-nightingale.txt', '7-the-song-of-achilles.txt', '8-atomic-habits.txt', '8-how-to-win-friends-and-influence-people.txt', '8-rich-dad-poor-dad.txt', '8-the-power-of-now.txt', '8-the-subtle-art-of-not-giving-a-fuck.txt', '9-jane-eyre.txt', '9-little-women.txt', '9-the-picture-of-dorian-gray.txt', '9-to-kill-a-mockingbird.txt']\n"
          ]
        }
      ],
      "source": [
        "def clean_text(line):\n",
        "    line = line.lower()\n",
        "    punctuation_allowed = \"'-\"\n",
        "    punctuation_to_remove = ''.join(c for c in string.punctuation if c not in punctuation_allowed)\n",
        "    line = line.translate(str.maketrans('', '', punctuation_to_remove))\n",
        "    line = re.findall(r\"\\b[a-zA-Z]+(?:['-][a-zA-Z]+)*\\b\", line)\n",
        "    line = [word[:-2] if word.endswith(\"'s\") else word for word in line]\n",
        "    return ' '.join(line)\n",
        "\n",
        "\n",
        "def clean_text_count(file_path, stopwords_path, name_path):\n",
        "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "        stopwords = set(word.strip().lower() for word in f.readlines())\n",
        "\n",
        "    with open(name_path, 'r', encoding='utf-8') as f:\n",
        "        names = set(word.strip().lower() for word in f.readlines())\n",
        "\n",
        "    word_counter = Counter()\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            cleaned_line = clean_text(line)\n",
        "            words = cleaned_line.split()\n",
        "            filtered = [word for word in words if word not in stopwords and word not in names]\n",
        "            filtered = [word for word in words if word not in stopwords]\n",
        "            word_counter.update(filtered)\n",
        "    return word_counter\n",
        "\n",
        "\n",
        "def build_matrix(folder_path, stopwords_path, name_path):\n",
        "    matrix = {}\n",
        "    vocabulary = set()\n",
        "\n",
        "    for file_path in Path(folder_path).glob(\"*.txt\"):\n",
        "        word_counts = clean_text_count(file_path, stopwords_path, name_path)\n",
        "        matrix[file_path.name] = word_counts\n",
        "        vocabulary.update(word_counts.keys())\n",
        "\n",
        "    matrix = {k: v for k, v in sorted(matrix.items())}\n",
        "    vocabulary = sorted(vocabulary)\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "    doc_names = list(matrix.keys())\n",
        "\n",
        "    term_doc_matrix = np.zeros((len(doc_names), len(vocabulary)), dtype=int)\n",
        "\n",
        "    for i, doc in enumerate(doc_names):\n",
        "        for word, count in matrix[doc].items():\n",
        "            j = vocab_index[word]\n",
        "            term_doc_matrix[i][j] = count\n",
        "\n",
        "    return doc_names, vocabulary, term_doc_matrix\n",
        "\n",
        "folder_path = \"data/documents\"\n",
        "stopwords_path = \"data/stop_words/stopwords.txt\"\n",
        "name_path = \"data/stop_words/names.txt\"\n",
        "\n",
        "doc_names, vocabulary, term_doc_matrix = build_matrix(folder_path, stopwords_path, name_path)\n",
        "print(doc_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "P2FTT9nq77uN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8eef27-0fe5-45d0-9d98-5948afdfe01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.45151438e-05  9.01901583e-06  3.07769626e-06 ... -3.01082765e-05\n",
            "   3.16908261e-05 -9.08617956e-05]\n",
            " [ 1.45151438e-05  9.01901583e-06  3.07769626e-06 ... -3.01082765e-05\n",
            "   3.16908261e-05 -9.08617956e-05]\n",
            " [ 1.45151438e-05  9.01901583e-06  3.07769626e-06 ... -3.01082765e-05\n",
            "   3.16908261e-05 -9.08617956e-05]\n",
            " ...\n",
            " [ 3.92165012e-06  4.00707967e-06 -1.49647252e-06 ... -2.58617862e-05\n",
            "   2.19318027e-05 -2.74879306e-05]\n",
            " [ 4.61611087e-05  1.33722087e-04  1.47134539e-04 ...  2.84680535e-08\n",
            "   1.81082352e-05 -5.18666165e-06]\n",
            " [ 6.78060790e-06  3.23927840e-06 -4.04333191e-06 ... -1.98619278e-05\n",
            "  -1.89520854e-06 -9.94818886e-06]]\n"
          ]
        }
      ],
      "source": [
        "def svd(matrix):\n",
        "    transpose = np.transpose(matrix)\n",
        "    work_matrix = np.dot(matrix, transpose)\n",
        "\n",
        "    eigenvalues, left_sing_matrix_U = np.linalg.eigh(work_matrix)\n",
        "\n",
        "    idx = np.argsort(eigenvalues)[::-1]\n",
        "    eigenvalues = eigenvalues[idx]\n",
        "    left_sing_matrix_U = left_sing_matrix_U[:, idx]\n",
        "\n",
        "    singular_values = np.sqrt(np.maximum(eigenvalues, 0))\n",
        "    sigma_E = np.diag(singular_values)\n",
        "\n",
        "    right_sing_matrix_V = []\n",
        "    for i in range(len(singular_values)):\n",
        "        if singular_values[i] > 1e-10:\n",
        "            v_i = np.dot(transpose, left_sing_matrix_U[:, i]) / singular_values[i]\n",
        "        else:\n",
        "            v_i = np.zeros(matrix.shape[0])\n",
        "        right_sing_matrix_V.append(v_i)\n",
        "    right_sing_matrix_V = np.column_stack(right_sing_matrix_V)\n",
        "\n",
        "    return left_sing_matrix_U, sigma_E, right_sing_matrix_V\n",
        "\n",
        "U, Sigma, V = svd(term_doc_matrix)\n",
        "print(V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "G7sT6QJh-mHY"
      },
      "outputs": [],
      "source": [
        "k = 10000\n",
        "def reduce(V, k):\n",
        "    Vt = V.T\n",
        "    reduced = Vt[:, :k]\n",
        "    return reduced\n",
        "reduced = reduce(V, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "FIq_MW2m_pRN"
      },
      "outputs": [],
      "source": [
        "def softmax(logits):\n",
        "    logits_shifted = logits - np.max(logits, axis=-1, keepdims=True)\n",
        "    exps = np.exp(logits_shifted)\n",
        "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "\n",
        "def predict(X, weights, bias, book_titles, index_to_label):\n",
        "    predictions = []\n",
        "    for xi, title in zip(X, book_titles):\n",
        "        z = np.dot(xi, weights) + bias\n",
        "        probs = softmax(z)\n",
        "        pred_idx = np.argmax(probs)\n",
        "        predicted_label = index_to_label[pred_idx]\n",
        "        predicted_prob = probs[pred_idx]\n",
        "        predictions.append((title, predicted_label, predicted_prob))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def gradient_descent(X, y_onehot, W, b, lr, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for xi, yi in zip(X, y_onehot):\n",
        "\n",
        "            z = np.dot(xi, W) + b\n",
        "            probs = softmax(z)\n",
        "\n",
        "            loss = -np.sum(yi * np.log(probs + 1e-8))\n",
        "            total_loss += loss\n",
        "\n",
        "            dz = probs - yi\n",
        "            dW = np.outer(xi, dz)\n",
        "            db = dz\n",
        "\n",
        "            W -= lr * dW\n",
        "            b -= lr * db\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    return W, b\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_labels = [\"fantasy\"] * 5 + [\"mystery\"] * 5 + [\"science fiction\"] * 6 + [\"horror\"] * 5 + [\"romance\"] * 5 + [\"adventure\"] * 5 + [\"historical fiction\"] * 5 + [\"self help\"] * 5 + [\"classics\"] * 5\n",
        "\n",
        "unique_labels = sorted(set(doc_labels))\n",
        "label_to_index = {l: i for i, l in enumerate(unique_labels)}\n",
        "index_to_label = {i: l for l, i in label_to_index.items()}\n",
        "print(index_to_label)\n",
        "y = np.array([label_to_index[l] for l in doc_labels])\n",
        "y_onehot = np.eye(len(unique_labels))[y]"
      ],
      "metadata": {
        "id": "zSTKz4fiqz7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b16620-f705-4b79-af4a-65353e8b1058"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'adventure', 1: 'classics', 2: 'fantasy', 3: 'historical fiction', 4: 'horror', 5: 'mystery', 6: 'romance', 7: 'science fiction', 8: 'self help'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swPs0bZ5BCnt",
        "outputId": "b808dc07-b126-4a2c-c914-3126044d2b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 99.9491\n",
            "Epoch 100, Loss: 44.5858\n",
            "Epoch 200, Loss: 26.1169\n",
            "Epoch 300, Loss: 17.9081\n",
            "Epoch 400, Loss: 13.5519\n",
            "Epoch 500, Loss: 10.8918\n",
            "Epoch 600, Loss: 9.0947\n",
            "Epoch 700, Loss: 7.7943\n",
            "Epoch 800, Loss: 6.8077\n",
            "Epoch 900, Loss: 6.0332\n",
            "Epoch 1000, Loss: 5.4093\n",
            "Epoch 1100, Loss: 4.8965\n",
            "Epoch 1200, Loss: 4.4679\n",
            "Epoch 1300, Loss: 4.1047\n",
            "Epoch 1400, Loss: 3.7933\n",
            "Epoch 1500, Loss: 3.5236\n",
            "Epoch 1600, Loss: 3.2879\n",
            "Epoch 1700, Loss: 3.0803\n",
            "Epoch 1800, Loss: 2.8961\n",
            "Epoch 1900, Loss: 2.7318\n",
            "Epoch 2000, Loss: 2.5843\n",
            "Epoch 2100, Loss: 2.4513\n",
            "Epoch 2200, Loss: 2.3307\n",
            "Epoch 2300, Loss: 2.2210\n",
            "Epoch 2400, Loss: 2.1207\n",
            "Epoch 2500, Loss: 2.0288\n",
            "Epoch 2600, Loss: 1.9442\n",
            "Epoch 2700, Loss: 1.8661\n",
            "Epoch 2800, Loss: 1.7938\n",
            "Epoch 2900, Loss: 1.7268\n",
            "Epoch 3000, Loss: 1.6644\n",
            "Epoch 3100, Loss: 1.6062\n",
            "Epoch 3200, Loss: 1.5518\n",
            "Epoch 3300, Loss: 1.5009\n",
            "Epoch 3400, Loss: 1.4531\n",
            "Epoch 3500, Loss: 1.4082\n",
            "Epoch 3600, Loss: 1.3658\n",
            "Epoch 3700, Loss: 1.3259\n",
            "Epoch 3800, Loss: 1.2882\n",
            "Epoch 3900, Loss: 1.2525\n",
            "Epoch 4000, Loss: 1.2187\n",
            "Epoch 4100, Loss: 1.1867\n",
            "Epoch 4200, Loss: 1.1562\n",
            "Epoch 4300, Loss: 1.1272\n",
            "Epoch 4400, Loss: 1.0996\n",
            "Epoch 4500, Loss: 1.0733\n",
            "Epoch 4600, Loss: 1.0482\n",
            "Epoch 4700, Loss: 1.0242\n",
            "Epoch 4800, Loss: 1.0013\n",
            "Epoch 4900, Loss: 0.9793\n",
            "Epoch 5000, Loss: 0.9583\n",
            "Epoch 5100, Loss: 0.9381\n",
            "Epoch 5200, Loss: 0.9188\n",
            "Epoch 5300, Loss: 0.9002\n",
            "Epoch 5400, Loss: 0.8824\n",
            "Epoch 5500, Loss: 0.8652\n",
            "Epoch 5600, Loss: 0.8486\n",
            "Epoch 5700, Loss: 0.8327\n",
            "Epoch 5800, Loss: 0.8174\n",
            "Epoch 5900, Loss: 0.8026\n",
            "Epoch 6000, Loss: 0.7883\n",
            "Epoch 6100, Loss: 0.7745\n",
            "Epoch 6200, Loss: 0.7612\n",
            "Epoch 6300, Loss: 0.7483\n",
            "Epoch 6400, Loss: 0.7358\n",
            "Epoch 6500, Loss: 0.7238\n",
            "Epoch 6600, Loss: 0.7121\n",
            "Epoch 6700, Loss: 0.7008\n",
            "Epoch 6800, Loss: 0.6898\n",
            "Epoch 6900, Loss: 0.6792\n",
            "Epoch 7000, Loss: 0.6689\n",
            "Epoch 7100, Loss: 0.6589\n",
            "Epoch 7200, Loss: 0.6491\n",
            "Epoch 7300, Loss: 0.6397\n",
            "Epoch 7400, Loss: 0.6305\n",
            "Epoch 7500, Loss: 0.6216\n",
            "Epoch 7600, Loss: 0.6130\n",
            "Epoch 7700, Loss: 0.6045\n",
            "Epoch 7800, Loss: 0.5963\n",
            "Epoch 7900, Loss: 0.5883\n",
            "Epoch 8000, Loss: 0.5805\n",
            "Epoch 8100, Loss: 0.5730\n",
            "Epoch 8200, Loss: 0.5656\n",
            "Epoch 8300, Loss: 0.5584\n",
            "Epoch 8400, Loss: 0.5513\n",
            "Epoch 8500, Loss: 0.5445\n",
            "Epoch 8600, Loss: 0.5378\n",
            "Epoch 8700, Loss: 0.5313\n",
            "Epoch 8800, Loss: 0.5249\n",
            "Epoch 8900, Loss: 0.5187\n",
            "Epoch 9000, Loss: 0.5126\n",
            "Epoch 9100, Loss: 0.5067\n",
            "Epoch 9200, Loss: 0.5009\n",
            "Epoch 9300, Loss: 0.4952\n",
            "Epoch 9400, Loss: 0.4897\n",
            "Epoch 9500, Loss: 0.4842\n",
            "Epoch 9600, Loss: 0.4789\n",
            "Epoch 9700, Loss: 0.4737\n",
            "Epoch 9800, Loss: 0.4686\n",
            "Epoch 9900, Loss: 0.4637\n"
          ]
        }
      ],
      "source": [
        "n_features = reduced.shape[1]\n",
        "n_classes = len(unique_labels)\n",
        "\n",
        "weights = np.random.randn(n_features, n_classes) * 0.01\n",
        "bias = np.zeros(n_classes)\n",
        "learning_rate = 0.2\n",
        "epochs = 10000\n",
        "weights, bias = gradient_descent(reduced, y_onehot, weights, bias, learning_rate, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n== Prediction on training data ==\")\n",
        "predictions = predict(reduced, weights, bias, doc_names, index_to_label)\n",
        "for title, predicted_label, predicted_prob in predictions:\n",
        "    print(f\"Book: {title}, Predicted Genre: {predicted_label}, Probability: {predicted_prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S176tDVPtcWc",
        "outputId": "66ca4f92-8948-4281-98ae-d53468ada641"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== Prediction on training data ==\n",
            "Book: 1-LOTR.txt, Predicted Genre: fantasy, Probability: 0.9976\n",
            "Book: 1-game-of-thrones.txt, Predicted Genre: fantasy, Probability: 0.9989\n",
            "Book: 1-harry-potter-and-the-chamber-of-secrets.txt, Predicted Genre: fantasy, Probability: 0.9989\n",
            "Book: 1-harry-potter-and-the-sorceres-stone.txt, Predicted Genre: fantasy, Probability: 0.9589\n",
            "Book: 1-the-lion-the-witch-and-the-wardrobe.txt, Predicted Genre: fantasy, Probability: 0.9170\n",
            "Book: 2-a-study-in-scarlet.txt, Predicted Genre: mystery, Probability: 0.9999\n",
            "Book: 2-and-then-there-were-none.txt, Predicted Genre: mystery, Probability: 1.0000\n",
            "Book: 2-murder-on-the-orient-express.txt, Predicted Genre: mystery, Probability: 0.9419\n",
            "Book: 2-the-girl-with-the-dragon-tattoo.txt, Predicted Genre: mystery, Probability: 0.9792\n",
            "Book: 2-the-hound-of-baskervilles.txt, Predicted Genre: mystery, Probability: 0.9900\n",
            "Book: 3-451-farenheit.txt, Predicted Genre: science fiction, Probability: 0.9935\n",
            "Book: 3-brave-new-world.txt, Predicted Genre: science fiction, Probability: 0.9903\n",
            "Book: 3-divergent.txt, Predicted Genre: science fiction, Probability: 0.9896\n",
            "Book: 3-dune.txt, Predicted Genre: science fiction, Probability: 0.9927\n",
            "Book: 3-enders-game.txt, Predicted Genre: science fiction, Probability: 0.9880\n",
            "Book: 3-the-martian.txt, Predicted Genre: science fiction, Probability: 1.0000\n",
            "Book: 4-carrie-stephen-king.txt, Predicted Genre: horror, Probability: 0.9888\n",
            "Book: 4-dracula.txt, Predicted Genre: horror, Probability: 0.9968\n",
            "Book: 4-frankenstein.txt, Predicted Genre: horror, Probability: 0.9949\n",
            "Book: 4-it-by-stephen-king.txt, Predicted Genre: horror, Probability: 0.9953\n",
            "Book: 4-the-shining.txt, Predicted Genre: horror, Probability: 0.9933\n",
            "Book: 5-persuasion.txt, Predicted Genre: romance, Probability: 0.9935\n",
            "Book: 5-pride-and-prejudice.txt, Predicted Genre: romance, Probability: 0.9884\n",
            "Book: 5-romeo-and-juliet.txt, Predicted Genre: romance, Probability: 0.9912\n",
            "Book: 5-twilight-new-moon.txt, Predicted Genre: romance, Probability: 0.9892\n",
            "Book: 5-twilight.txt, Predicted Genre: romance, Probability: 0.9933\n",
            "Book: 6-moby-dick.txt, Predicted Genre: adventure, Probability: 0.9968\n",
            "Book: 6-the-count-of-monte-cristo.txt, Predicted Genre: adventure, Probability: 0.9953\n",
            "Book: 6-the-three-musketeers.txt, Predicted Genre: adventure, Probability: 0.9922\n",
            "Book: 6-tom-sawyer.txt, Predicted Genre: adventure, Probability: 0.9977\n",
            "Book: 6-treasure-island.txt, Predicted Genre: adventure, Probability: 0.9946\n",
            "Book: 7-a-tale-of-two-cities.txt, Predicted Genre: historical fiction, Probability: 0.9901\n",
            "Book: 7-memoirs-of-a-geisha.txt, Predicted Genre: historical fiction, Probability: 0.9927\n",
            "Book: 7-the-book-thief.txt, Predicted Genre: historical fiction, Probability: 0.9959\n",
            "Book: 7-the-nightingale.txt, Predicted Genre: historical fiction, Probability: 0.9948\n",
            "Book: 7-the-song-of-achilles.txt, Predicted Genre: historical fiction, Probability: 0.9975\n",
            "Book: 8-atomic-habits.txt, Predicted Genre: self help, Probability: 0.9958\n",
            "Book: 8-how-to-win-friends-and-influence-people.txt, Predicted Genre: self help, Probability: 0.9963\n",
            "Book: 8-rich-dad-poor-dad.txt, Predicted Genre: self help, Probability: 0.9902\n",
            "Book: 8-the-power-of-now.txt, Predicted Genre: self help, Probability: 0.9874\n",
            "Book: 8-the-subtle-art-of-not-giving-a-fuck.txt, Predicted Genre: self help, Probability: 0.9962\n",
            "Book: 9-jane-eyre.txt, Predicted Genre: classics, Probability: 0.9943\n",
            "Book: 9-little-women.txt, Predicted Genre: classics, Probability: 0.9941\n",
            "Book: 9-the-picture-of-dorian-gray.txt, Predicted Genre: classics, Probability: 0.9936\n",
            "Book: 9-to-kill-a-mockingbird.txt, Predicted Genre: classics, Probability: 0.9939\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}